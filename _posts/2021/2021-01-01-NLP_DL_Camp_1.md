---
title: "[김기현의 자연어 처리 딥러닝 캠프]1장. 딥러닝을 활용한 자연어 처리 개요"
categories:
  - 김기현의 자연어 처리 딥러닝 캠프
tags:
  - Natural_Language_processing
  - Machine_Learning
  - Deep_Learning
  - 자연어처리
  - 머신러닝
  - 딥러닝 
  - 공부
  - 책정리
last_modified_at: 2021-01-01T18:00:00+09:00
toc: true
---

 해당 카테고리에서는 **김기현의 자연어 처리 딥러닝 캠프 파이토치편**을 공부하면서 기억해 둘 것을 정리해논 곳 입니다.  
 이 포스트에는 **1장 딥러닝을 활용한 자연어 처리 개요**를 공부하며 정리한 것을 작성해 두었습니다.  

# 자연어처리란
**자연어**란 우리가 일상 생활에서 사용하는 언어를 말한다.  
**자연어 처리(Natural Language Processing, NLP)**는 인공지능의 한 분야로, 사람의 언어를 컴퓨터가 알아듣도록 처리하는 일을 말한다.  

# 딥러닝
딥러닝의 발전으로 머신러닝의 분야들 거의 모두에 많은 영향을 미침  
아래 논문들은 각 분야의 딥러닝 적용의 시작 혹은 중요 기점이 되는 논문들

### 이미지 분류 (Image Classification)
**AlexNet** : [ImageNet Classification with Deep Convolutional Neural Networks](https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)  
**ResNet** : [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)  

### 음성 인식(Speech Recognition)
**End-to-end 방식** : [ State-of-the-art Speech Recognition With Sequence-to-Sequence Models](https://arxiv.org/abs/1712.01769)  

### 기계번역(Machine Translation)
**Attention Mechanism** : [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)  

### 생성 모델 학습(Generative Model Learning)
**Adversarial Learning** : [Generative Adversarial Networks](https://arxiv.org/abs/1406.2661)  
**Variation Autoencoder, VAE** : [ Auto-Encoding Variational Bayes](https://arxiv.org/abs/1312.6114)  

### 자연어처리
[딥러닝 자연어처리 요약](https://www.microsoft.com/en-us/research/wp-content/uploads/2017/07/dl-summer-school-2017.-Jianfeng-Gao.v2.pdf)

# 자연어처리의 어려운점
1. 모호성 - 문장 내 정보 부족으로 인한 다양한 해석의 가능
2. 다양한 표현 - 한 상황에 대하여 여러 다향한 표현이 가능
3. 불연속적 데이터 - 딥러닝은 연속적인 값이 필요함
4. 한국어의 어려운점 - 교착어, 띄어쓰기, 평서문, 의문문, 주어 생략, 한자 기반의 언어

# 자연어처리의 최근 추세
**word2vec** : 단어들을 잠재공간(latent space)에 투사시켜 딥러닝을 효과적으로 사용할 수 있게 만듬. [Distributed Representations of Words and Phrases and their Compositionality](https://papers.nips.cc/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html)    
**CNN 텍스트 분류 모델** : 기존 RNN만 사용해야 한다는 고정관념 타파. [Convolutional Neural Networks for Sentence Classification](https://arxiv.org/abs/1408.5882)    
**seq2seq** : 자연어 처리 분야의 큰 혁명 논문. [Sequence to Sequence Learning with Neural Networks](https://papers.nips.cc/paper/2014/hash/a14ac55a4f27472c5d894ec1c3c743d2-Abstract.html)  
**Attention Mechanism** : [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)    
**Neural Turing Machine, NTM** : 메모리를 활용한 심화 연구. [Neural Turing Machines](https://arxiv.org/abs/1410.5401)  
**Differential Neural Computer, DNC** : 메모리를 활용한 심화 연구. [Hybrid computing using a neural network with dynamic external memory](https://www.nature.com/articles/nature20101?curator=TechREDEF)  


> Written with [StackEdit](https://stackedit.io/).
